{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING: Day 2\n",
    "\n",
    "The following contains coding examples used in the second day of the CALDISS workshop: \"Web Scraping for the Social Sciences\".\n",
    "\n",
    "The following is meant to introduce basic web scraping in python.\n",
    "\n",
    "Copy the code to your own folder to run and edit the code yourself.\n",
    "\n",
    "**CONTENT**\n",
    "- Accessing the web with `requests`\n",
    "- Working with HTML in `Selector` objects from `scrapy`\n",
    "- Navigating HTML with XPATH\n",
    "- Parsing HTML with `scrapy`\n",
    "- Creating a \"spider\" with `scrapy`\n",
    "- Understanding JSON formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the web\n",
    "\n",
    "Accessing a website in python can be done using the `requests` library and creating a `response` object.\n",
    "\n",
    "The HTML content can be extracted from the `response` object.\n",
    "\n",
    "Parsing the HTML and extracting specific tags requires addional libraries being set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "caldiss_url = 'https://www.en.caldiss.aau.dk/about/'\n",
    "caldiss_get = requests.get(caldiss_url)\n",
    "print(str(caldiss_get.status_code) + ' ' + caldiss_get.reason)  #Status code 200 is \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `response` object we can extract the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caldiss_html = caldiss_get.content\n",
    "print(caldiss_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_suburl = 'https://www.en.caldiss.aau.dk/allaboutmeanandpastas/'\n",
    "invalid_subget = requests.get(invalid_suburl)\n",
    "print(str(invalid_subget.status_code) + ' ' + invalid_subget.reason)  #Status code 404 is \"Not found\" - Useful for error handling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sites have a page setup for 404 errors (so that users know that they are on the right main site).\n",
    "\n",
    "We can still therefore still extract the HTML (the HTML of the 404 page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_subhtml = invalid_subget.content\n",
    "print(invalid_subhtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief look at HTML\n",
    "\n",
    "- HTML consists of tags <>\n",
    "- The tags are always in hierarchical structure - like a tree\n",
    "\n",
    "Below is a HTML string example in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p class=\"kenobi\">Hello There!</p>\n",
    "      <div>\n",
    "        <p>General Kenobi!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>So Uncivilized!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Tags\n",
    "\n",
    "- Tags are used to indicate elements\n",
    "- Tags can contain attributes: \n",
    "    - Classes: `<div class=\"class!\">`\n",
    "    - Id's: `<div id=\"div2\">`\n",
    "    - id's should be unique\n",
    "    - classes do not need to be unique\n",
    "    - Examples of attributes: href, rel, type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating HTML: XPATH Notation\n",
    "\n",
    "XPATH notation is one way of specifying the path to specific HTML element(s).\n",
    "\n",
    "An XPATH consists of a path-string pointing to spceific HTML elements.\n",
    "\n",
    "Example: `/html/body/div[1]/p`\n",
    "The above XPATH extracts the element `p` under the first `div` element under the `body` elements under the `html` element.\n",
    "\n",
    "### Basic vocabulary\n",
    "\n",
    "- `/` (forward slash): move forward one generation (in the tree structure)\n",
    "- `[]` (brackets): sibling index (after a tag)\n",
    "\t- Index can also be specified with an attribute: `[@id=\"uid\"]` (attribute id = \"uid\")\n",
    "\t- NOTE: Index 1 will take all the first siblings of a tag across generations (first siblings across different hierarchical levels)\n",
    "- `//` (double forward slag): all elements with specific tag\n",
    "\t- Can also call all elements with a specific tag at a specific level\n",
    "- `*` (wildcard)\n",
    "    - all children: `/*` \n",
    "    - all descendants: `//*`\n",
    "- `@`: specify attributes\n",
    "    - Attributes can be navigated to by specifying `'/@href'` in the xpath\n",
    "\t- Can be combined with wildcards (find all elements with a specific attribute)\n",
    "\t- `contains`: function to use regular expression to find all attributes containing expression \n",
    "        - `'//*[contains(@class, \"class-1\")]'`: finds all class attributes containing \"class-1\" (NOTE: \"class-12\" also meets this criteria)\n",
    "- `text()`: extract the textual content (excluding html tags)\n",
    "\n",
    "Given the html string above, the strings below are all xpaths leading to \"Hello There!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath1 = '/html/body/div[1]/p'  #Specific path directory\n",
    "xpath2 = '//p[@class=\"kenobi\"]'  #Specified by class\n",
    "xpath3 = '/html/body//*[contains(@class, \"kenobi\")]'  #Wildcard - all elements with classes containing the word \"kenobi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `scrapy` to extract HTML elements\n",
    "\n",
    "The package `scrapy` is specifically for web scraping.\n",
    "\n",
    "It works by creating a `Selector` object from site's HTML. Elements from the object can be extracted using xpaths.\n",
    "\n",
    "Using `.extract` or `.extract_first()` extracts all and the first element respectively as python objects (list or single string). Otherwise the object extracted is stored as a `Selector` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting text\n",
    "\n",
    "The code below demonstrates how to extract the raw text of an HTML element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "aau_url = \"https://www.en.caldiss.aau.dk/\"  #The URL we want to scrape\n",
    "aau_html = requests.get(aau_url).content  #Extract the content (the HTML) using requests (as seen earlier)\n",
    "\n",
    "aau_sel = Selector(text = aau_html)  #convert html to Selector object\n",
    "\n",
    "title_xpath = \"//title/text()\"  #the xpath - find all \"title\" tags and extract the text\n",
    "\n",
    "aau_title = aau_sel.xpath(title_xpath).extract_first()  #Extract the text of the first title element using xpath above\n",
    "\n",
    "print(aau_title)  #Print the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting attributes\n",
    "\n",
    "The code below demonstrates how to extract attributes (here links).\n",
    "\n",
    "(the code uses the object `aau_sel` from above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "url_xpath = '//@href'  #xpath to all href attributes (links)\n",
    "\n",
    "aau_urls = aau_sel.xpath(url_xpath).extract()  #Extract all links with xpath above\n",
    "\n",
    "aau_urls  #Print the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we are extracting every link on the page. Often we are interested in links appearing a specific place on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "url_xpath = '//ul[@class=\"unstyled\"]//@href'  #xpath to links appearing under ul elements of class \"unstyled\"\n",
    "\n",
    "aau_urls = aau_sel.xpath(url_xpath).extract()  #Extract all links with xpath above\n",
    "\n",
    "aau_urls  #Print the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Parsing HTML\n",
    "\n",
    "Reusing code from above, extract the text of all `h2` elements that are siblings of `div` elements of class \"article\" from the site: https://www.en.caldiss.aau.dk/about/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "\n",
    "aau_url = #????#  #The URL we want to scrape\n",
    "\n",
    "aau_html = requests.get(aau_url).content  \n",
    "\n",
    "aau_sel = Selector(text = aau_html)  \n",
    "\n",
    "h2_xpath = #????#  #The xpath to extract the elements\n",
    "\n",
    "aau_text = aau_sel.xpath(h2_xpath).extract()  \n",
    "\n",
    "print(aau_text)  #Print the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a spider with `scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can scrape all kinds of content from a site, we can also scrape the URL's a site is linking to.\n",
    "\n",
    "This allows us to create scrapers, that extract specific content from URL's and then jumping on to URL's within those URL's.\n",
    "\n",
    "Scrapers that are meant to \"crawl\" from site to site are also called \"spiders\".\n",
    "\n",
    "The `scrapy` packages contain build-in functions and classes for setting up a spider. An example is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spider-example: Extracting paragraphs from all subpages of the CALDISS page\n",
    "\n",
    "Using the URL's extracted above, we can create a simple spider to go through each of these URL's and extract specific information.\n",
    "\n",
    "The scrape provided us with some irrelevant URL's. First we clean up a bit to only have the URL's we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_url = aau_urls.index(\"/contact/\")\n",
    "\n",
    "caldiss_urls = aau_urls[0:last_url]\n",
    "caldiss_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL's only contain the suffix of the URL. We can simply paste the correct prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caldiss_urls = [\"https://www.en.caldiss.aau.dk\" + url for url in caldiss_urls]\n",
    "caldiss_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create the spider using the URL's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class CALDISS_spider(scrapy.Spider):\n",
    "    name = \"caldiss_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = caldiss_urls\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse)\n",
    "\n",
    "    # First parsing method\n",
    "    def parse( self, response ): # Functiong for parsing html and writing it to a file - defines the parse method used above (callback)\n",
    "        print(response.url)\n",
    "        page_url = response.url\n",
    "        page_html = requests.get(page_url).content  #html from url\n",
    "        page_sel = Selector(text = page_html)  #create selector object\n",
    "\n",
    "        para_xpath = \"//p/text()\"  #xpath for extracting text of all paragraphs elements\n",
    "        \n",
    "        page_paras = page_sel.xpath(para_xpath).extract()  #extract the text\n",
    "\n",
    "        caldiss_dict[ page_url ] = ' '.join([para.lower() for para in page_paras])  #join the paragraphs and add to dictionary\n",
    "        \n",
    "        \n",
    "# Create an empty dictionary\n",
    "caldiss_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(CALDISS_spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caldiss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common file formats on the web: JSON\n",
    "\n",
    "A common formatting of data on the web is the JSON format (JavaScript Object Notation).\n",
    "\n",
    "JSON is a hierarchical type of format where data is stored in attribute-value pairs.\n",
    "\n",
    "An example of text formatted as JSON can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_example = '{\"names\": [{\"value\": \"kenobi\"}, {\"value\": \"kylo\"}, {\"value\": \"windu\"}], \"rank\": [{\"title\": \"jedi master\"}, {\"title\": \"sith apprentice\"}, {\"title\": \"jedi master\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `json` package can convert JSON formats to python-readable format. The most \"direct\" translation is a conversion to a python dictionary (which also is a attribute-value pair format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_obj = json.loads(json_example)  #Read the JSON-formatted data as a dictionary.\n",
    "print(json.dumps(json_obj, indent = 1))  #Pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON can now be subset like a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in json_obj[\"names\"]:\n",
    "    print(name.get(\"value\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in a JSON (an object) can contain sub-elements. \n",
    "\n",
    "When using the twitter API, each tweet is an object with a lot of elements (text, id, time, author etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Headers\n",
    "\n",
    "Custom HTTP headers can be set when accessing a URL via python. This makes it possible to leave contact information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0 Win64 x64 rv:66.0); Kristian Gade Kjelmann/Aalborg University/kgk@adm.aau.dk'}\n",
    "\n",
    "aau_url = \"https://www.aau.dk\"\n",
    "aau_html = requests.get(aau_url, headers=headers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
