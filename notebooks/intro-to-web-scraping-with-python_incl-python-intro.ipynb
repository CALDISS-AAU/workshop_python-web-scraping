{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web scraping with Python\n",
    "\n",
    "## Content\n",
    "\n",
    "- Hvad er web scraping?\n",
    "    - Relevansen af web scraping\n",
    "- Introduktion til internettet\n",
    "    - Internettet og \"webbet\" (kort fortalt)\n",
    "    - Hvordan virker internettet? (kort fortalt)\n",
    "    - Hvad består hjemmesider af? (HTML)\n",
    "- \"Legaliteten\" af web scraping\n",
    "    - Web scraping og copyright\n",
    "    - Web scraping og persondata\n",
    "    - Web scraping og \"hacking\"\n",
    "- Eksempel på web scraping med Python (mål for dagen)\n",
    "- Kort introduktion til Python\n",
    "    - Python sproget i Jupyter Notebook\n",
    "    - Variable i Python\n",
    "    - Pakker og Python funktionalitet\n",
    "- Python og internettet\n",
    "    - Internettet gennem Python (requests)\n",
    "    - Klasser og attributter (response object)\n",
    "- Naviger i HTML med Python\n",
    "    - Kort introuktion til HTML\n",
    "    - Intro til BeautifulSoup\n",
    "    - Soup-klassen\n",
    "    - Find HTML gennem tags og attributter\n",
    "    - Python lister (find_all)\n",
    "    - Naviger i HTML strukturen\n",
    "- Øvelse: Hent overskrifter fra nyheder.tv2.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is web scraping?\n",
    "\n",
    "\"Web scraping\" is an umbrella term for (mostly) automated techniques for collecting information from the web (usually refers to collection *not* done manually in a browser).\n",
    "\n",
    "Crawling, scraping and spiders are all various forms of scraping. Programs/scripts written for scraping can also be refered to as robots.\n",
    "\n",
    "Working with web scraping involves the collection of raw data from the web as well as handling and converting these data to a manageable and analyzable format.\n",
    "\n",
    "### Why is web scraping relevant? (for the social sciences)\n",
    "\n",
    "**1. The internet and web is a relevant field of study in itself**\n",
    "\n",
    "The web is completely interwoven with our daily lives. Information on the web and our use of the web provide insight into habits, consumption and interaction.\n",
    "\n",
    "**2. The internet and web is a data source**\n",
    "\n",
    "The web is a massive collection of information. We need techniques for systematic collection of this information\n",
    "\n",
    "\n",
    "This workshop focuses mostly on the web as a data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to the internet\n",
    "\n",
    "### The internet and the web (in a nutshell)\n",
    "\n",
    "To be able to work with the web, we need some fundamental understanding of what the web and the internet actually are.\n",
    "\n",
    "Terms like \"the internet\" and \"the web\" are often used interchangeably (especially in Danish where there is not really a term for \"the web\").  They do however refer to different parts of a larger infrastructure.\n",
    "\n",
    "**The internet**: A global system of interconnected computers and servers.\n",
    "- \"Internet\" is short for \"interconnected network\"\n",
    "\n",
    "**(World Wide) Web (WWW)**: A collection of resources made available via the internet\n",
    "- \"The web\" can be considered as the navigable and public parts of the internet\n",
    "\n",
    "**IP**: Short for \"internet protocol\" - a unique address on the internet\n",
    "- Every computer and server on the internet has a unique id in the form of an IP-address (fx 127.28.115.253)\n",
    "\n",
    "**URL**: Unique and human readable addresses for servers on the internet \n",
    "- Short for \"uniform ressource locators\"\n",
    "\n",
    "**Trafic on the internet**\n",
    "- When accessing the internet via the web, other computers and servers are contacted for information\n",
    "- The connections between IP-addresses and URL's are handled by DNS servers (DNS: Domain Name System)\n",
    "\n",
    "<img src=\"img/internet.png\" alt=\"opte\" width=\"600\"/>\n",
    "\n",
    "*Opte Project 2005*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the web works (in a nutshell)\n",
    "\n",
    "![dns](img/dns.png)\n",
    "\n",
    "*wpbeginner.com*\n",
    "\n",
    "*As a consequence of this there is not common \"address book\" of the web*\n",
    "\n",
    "We are either dependent on existing scrapers (fx [the wayback machine](https://web.archive.org/)) or our own crawlers for collecting data on the web.\n",
    "\n",
    "### Content on the web\n",
    "\n",
    "- Source code (raw and rendered)\n",
    "- Media files (pictures, videos, etc.)\n",
    "- Dynamic content (fx JavaScript, Ajax)\n",
    "- Content from other websites\n",
    "\n",
    "[demonstrate network tool in browser]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"legality\" of web scraping\n",
    "\n",
    "### Web scraping and copyright\n",
    "\n",
    "The information and material on company and organisation websites are owned by those companies and organisations. A lot of websites have terms of use that prohibit the use of scraping or automated methods of collection on their websites. This is partly due to the fact that one can create a website with scrapers that duplicate the content of other websites. Even though this may not be our intention with the scraping that we do, it will often be considered a violation of the website's terms.\n",
    "\n",
    "### Web scraping and personal data\n",
    "\n",
    "Data on social media are a particular grey area when it comes to personal data. Is data that people voluntarily make available on public social media profiles still their data? A website like Twitter for example clearly states that all data made available on their platform is to be considered public information. Even so, a case could still be made that the data is still personal; just publicly available personal data.\n",
    "\n",
    "Websites like Facebook, Twitter and Instagram also have terms of use for their data as they are responsible for it. Facebook and Instagram do for exmaple not allow any forms of automated collection of data.\n",
    "\n",
    "### Web scraping og \"hacking\"\n",
    "\n",
    "A website is located on a server. Each time a website is visited, a server is receiving a request to be processed. The more requests, the more busy the server is. Python allows us to easily write commands that send an incredible amount of requests within very short time.\n",
    "\n",
    "*This can easily be considered an attack and an attempt to congest a server which is illegal!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksempel på web scraping med Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02c360dff2d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_content = requests.get(\"https://nyheder.tv2.dk/\").content\n",
    "\n",
    "tv2_soup = bs(url_content)\n",
    "texts = tv2_soup.find_all(\"a\", class_ = \"o-teaser_link\")\n",
    "texts = [text.get_text() for text in texts]\n",
    "text = ''.join(texts)\n",
    "\n",
    "doc = nlp(text)\n",
    "words = []\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "stopwords = list(nlp.Defaults.stop_words) + [\"LIVE\"]\n",
    "punctuation = string.punctuation + \"”\"\n",
    "\n",
    "for token in doc:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "\n",
    "    if(token.pos_ in pos_tag):\n",
    "        words.append(token.text)\n",
    "        \n",
    "counts = pd.Series(words).value_counts()\n",
    "counts = counts[counts > 1]\n",
    "\n",
    "wc = WordCloud(background_color=\"white\")\n",
    "wc.fit_words(counts)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction to Python\n",
    "\n",
    "Python is a **general purpose** programming language. This means that it is developed with an emphasis on being compatible with a wide variety of tasks. Web scraping is one of the many things that we can use Python for.\n",
    "\n",
    "The main component of a programming language (and Python) is an **interpreter**. An \"interpreter\" in a programming language **evaluates** commands (tries to understand commands written in a certain language) and produces some **output** dependent on the commands. The interpreter either understands the command or throws an error.\n",
    "\n",
    "Python is also an **object-oriented** programming language. This means that writing Python code involves the continous creation of \"objects\" (or \"variables\"). \"Obejcts\" are basically containers for some kind of information to be refered to later. This means that objects can be pretty much anything from single digits, texts and URL's to collections of websites, datasets and so on.\n",
    "\n",
    "Python differentiates between objects via the object's **class** and **type**. Each object in Python is a certain class that contains certain **attributes** and **methods**. \n",
    "\n",
    "One works and manipulates objects in Python via the use of **functions** and **methods**.\n",
    "\n",
    "Python is **open-source** meaning that anyone can develop functions or classes for Python. Other functions and classes can be imported via **\"packages\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Python language (in Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Python code in a code cell in Jupyter Notebook, the code is evaluated. If the code is \"understood\", some output is produced. If not, Python throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(42 + 50) # Valid Python command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bake_cake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-353f19e1c525>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbake_cake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Invalid Python command - produces error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bake_cake' is not defined"
     ]
    }
   ],
   "source": [
    "bake_cake() # Invalid Python command - produces error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects/variables in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects/variables are assigned using `=` with the object name to the left and the content to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 42\n",
    "b = 90\n",
    "c = \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When objects are assigned they can be called and used later in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there\n"
     ]
    }
   ],
   "source": [
    "print(c, \"there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an object does not change the object. An object can usually only be changed through re-assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(a + 2) # Printing the result of a + 2\n",
    "print(a) # a is still 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions in Python\n",
    "\n",
    "Functions in Python follow this standard syntax: `functionname(arguement1, arguement2, ...)`\n",
    "\n",
    "That is: function name followed by parenthesis with the input arguement separated by commas.\n",
    "\n",
    "**\"Arguements\"** are the input for a function. A function can have any number of arguements from 0 to a lot. Valid arguements depend on the function. Standard arguements are usually some kind of input: some object that the function has to do something with.\n",
    "\n",
    "Most functions also contain \"keyword arguements\": `functionname(arguement1, arguement2, keywordarguement1 = True)`\n",
    "\n",
    "**\"Keyword arguements\"** can be considered the options of a function. Usually they accept the values `True` or `False` (for turning the option on or off) or accept a limited number of predefined text inputs.\n",
    "\n",
    "`print` is a function taking various text-coercible objects as arguement and prints them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\", \"there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print` has the keyword arguement `sep` deciding what should separate the input objects. The standard is a white space \" \" but it can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, there\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\", \"there\", sep = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Python funktionality\n",
    "\n",
    "Additional functions can be imported in Python via packages. \n",
    "\n",
    "Fx Python does not know the value of pi by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9e2d2bd32686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pi' is not defined"
     ]
    }
   ],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the value of pi is contained in the package `math` which can be imported into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing, one can use abbreviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "\n",
    "print(m.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If only specific parts of a package is to be used, those specific elements can be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and the internet\n",
    "\n",
    "When we access the web via a browser, the browser is doing a lot of work that we may not be aware of. \n",
    "\n",
    "All websites consist of some source code (usually HTML). Our browser renders this source code and displays a human-readable website with text formatting, images, links interactable elements and so on.\n",
    "\n",
    "When accessing websites in Python, we are getting the raw source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the internet with Python using `requests`\n",
    "\n",
    "The package `requests` is used to send requests to a website in Python (access the website).\n",
    "\n",
    "In the code below, we import the package `requests` and the send a \"get\" request to [https://nyheder.tv2.dk/](https://nyheder.tv2.dk/) using the function `requests.get` (when accessing a URL in a browser, our browser is also sending get requests). The request is stored in the object `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://nyheder.tv2.dk/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that no output is given as we are only asking for the object to be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and attributes (with examples using the response object)\n",
    "\n",
    "As mentioned, objects are always a certain class and type. Classes are associated with various methods and attributes that can be accessed using `.[attributename]` or `.[methodname]` (excluding the `[]`).\n",
    "\n",
    "The object `response` is a `Response` class. We can see this using the function `type()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sending a `get` requests to a website, the server we are contacting returns various kinds of information including the status code for the request, whether the request was successful, the so-called \"headers\" used for sending the request, the website content and so on.\n",
    "\n",
    "All this information is stored in various attributes in the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the request\n",
    "\n",
    "To check if the request was successful, we can check the status code by inspecting the attribute `.status_code`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status code 200 means \"OK\"; that our request was succesul. \n",
    "\n",
    "This can be verified by checking the attribute `.reason`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick note on status codes**\n",
    "\n",
    "- Status codes beginning with 2 or 3: The request is successful\n",
    "- Status codes beginning with 4: The request has failed (client-side, fx 404 when specifying a URL that does not exist on a given domain).\n",
    "- Status codes beginning with 5: The request has failed (server-side)\n",
    "\n",
    "Status codes can be used in code to check whether or not a site is reached before scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content of a webpage\n",
    "\n",
    "The raw source code from a webpage can be extracted from the attribute `.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"da-DK\" class=\"site-nyheder\">\\n<head>\\n\\n  <meta charset=\"utf-8\"/>\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"/>\\n\\n  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0\"/>\\n  <meta name=\"msapplication-TileImage\" content=\"https://shared.tv2.dk/sites/all/themes/t2/img/apple-touch-icon-precomposed.png\"/>\\n  <meta name=\"msapplication-TileColor\" content=\"#fff\"/>\\n  <meta name=\"format-detection\" content=\"telephone=no\"/>\\n  <link rel=\"apple-touch-icon-precomposed\" href=\"https://shared.tv2.dk/sites/all/themes/t2/img/apple-touch-icon-precomposed.png\"/>\\n  <link rel=\"shortcut icon\" href=\"https://shared.tv2.dk/favicon.ico\" />\\n\\n  <!-- inline base CSS -->\\n  <style type=\"text/css\" media=\"screen\">\\n    @charset \"UTF-8\";@font-face{font-family:AlrightSansLT-Regular;src:url(https://shared.tv2.dk/sites/all/themes/t2/fonts/AlrightSansLT-Regular.woff2) format(\"woff2\"),url(https://shared.tv2.dk/sites/all/themes/t2/fonts/AlrightSansLT-Regular.woff) format(\"woff\")}@'\n"
     ]
    }
   ],
   "source": [
    "content = response.content\n",
    "print(content[0:1000]) # Printing first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this raw source code, one *could* process this as is using something like regular expression to find the relevant parts of the source code.\n",
    "\n",
    "However, HTML has a certain structure. This can be utilized to extract specific information from a webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating HTML with Python\n",
    "\n",
    "Most websites are build with HTML source code. [https://nyheder.tv2.dk/](https://nyheder.tv2.dk/) is no exception as can be seen in the printed content above.\n",
    "\n",
    "In order to collect the relevant information from a website, we need to parse the HTML. \n",
    "\n",
    "HTML has a certain logic and structure that can be utilized to extract specific elements. Python has different packages for doing this. In this workshop we will be using the package `BeautifulSoup` ([https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)).\n",
    "\n",
    "### A short introduction to HTML\n",
    "\n",
    "HTML is short for \"Hyper-Text Markup Language\". It is used on webpages to give the pages their structure.\n",
    "\n",
    "HTML is structured in \"tags\" denoted by `<>` and `</>`. The tags denote what kind of content it is. `<p>` is for example a paragraph tag. A piece of HTML like: `<p> This is a paragraph </p>` will render the sentence \"This is a paragraph\" as a paragraph. Common tags include `h1` for headings (and `h2`, `h3` and so on), `a` for links and `div` for a \"division\" or \"section\".\n",
    "\n",
    "HTML is structured in a tree-like structure. Tags are therefore usually located within other tags. Tags on the same level are refered to as \"siblings\", tags inside other tags are refered to as \"children\" and tags outside other tags are refered to as \"parents\".\n",
    "\n",
    "HTML uses \"attributes\" to both differentiate between the same type of tags and to add other variables/information to the tag. The `id` attribute is fx used to give several tags a common id. `class` is used to differentiate between different tags and provide them with different stylings. A common and useful attribute is `href` which contain the link that a hyperlink is refering to.\n",
    "\n",
    "```\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id=\"convo1\">\n",
    "                <p class=\"kenobi\">Hello There!</p>\n",
    "            </div>\n",
    "            <div id=\"convo2\">\n",
    "                <p class=\"grievous\">General Kenobi!</p>\n",
    "            </div>\n",
    "            <div id=\"convo3\">\n",
    "                <p class=\"kenobi\">So Uncivilized!</p>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "```    \n",
    "\n",
    "\n",
    "The code above is an example of HTML code. Rendered as a webpage it would only contain the text within the tags:\n",
    "\n",
    "```\n",
    "Hello There!\n",
    "\n",
    "General Kenobi!\n",
    "\n",
    "So Uncivilized!\n",
    "```\n",
    "\n",
    "The structure and the tags of the HTML allows us to extract only specific parts of the code. This is because the structure and the tags makes certain part of the code uniquely identifiable. For example:\n",
    "\n",
    "- The text \"Hello There!\" is located within a p tag with the class \"kenobi\". \n",
    "- The p tag containing the text \"Hello There!\" is located within the div tag with id \"convo1\" (tags located inside other tags are refered to as \"children\")\n",
    "- The div tag with id \"convo1\" is located next to another div tag with id \"convo2\" (tags located next to each other or on the same level are refered to as \"siblings\")\n",
    "\n",
    "Combining the information, we can uniquely refer to the tag containing \"Hello There!\" by specifying that we want a p tag with class \"kenobi\" that is a child of a div tag with id \"convo1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the package `BeautifulSoup`\n",
    "\n",
    "The package \"BeautifulSoup\" (https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is developed specifically to navigate and parsing HTML (and XML) code. It works by converting HTML code to a \"soup-object\" wherein specific parts of the HTML can be extracted by refering to specific tags or paths.\n",
    "\n",
    "The code below converts the HTML from before to a soup object using the function `bs`, which is a shorthand for the function `BeautifulSoup` imported from `bs4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div id=\"convo1\">\n",
      "   <p class=\"kenobi\">\n",
      "    Hello There!\n",
      "   </p>\n",
      "  </div>\n",
      "  <div id=\"convo2\">\n",
      "   <p class=\"grievous\">\n",
      "    General Kenobi!\n",
      "   </p>\n",
      "  </div>\n",
      "  <div id=\"convo3\">\n",
      "   <p class=\"kenobi\">\n",
      "    So Uncivilized!\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "html = '<html><body><div id=\"convo1\"><p class=\"kenobi\">Hello There!</p></div><div id=\"convo2\"><p class=\"grievous\">General Kenobi!</p></div><div id=\"convo3\"><p class=\"kenobi\">So Uncivilized!</p></div></body></html>'\n",
    "soup = bs(html, \"html.parser\") # The second arguement specifies the parser to use; how the code should be interpreted\n",
    "print(soup.prettify()) # Prints the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When printed with `.prettify()` it looks like the same text but we are now able to navigate it using the tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `soup` class\n",
    "\n",
    "Using the `bs` function on HTML creates a `soup` class object. A `soup` object makes HTML navigable allowing us to find specific parts of a website by specifying HTML tags and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding HTML using tags\n",
    "\n",
    "The methods `.find()` and `.find_all()` are used to find the first match and all matches respectively. The first argument of the method is the tag. Other arguments can then be added to make the search more specific.\n",
    "\n",
    "Note that `.find()` and `.find_all()` are methods tied to a soup object, so they have to be used with some object returned from `bs` (in this case the object `soup` created earlier).\n",
    "\n",
    "`.find()` returns a new soup object with the HTML in the first matched tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"kenobi\">Hello There!</p>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"p\") # Finds the first p tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.find_all()` returns a list of soup objects with the HTML in the matched tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"kenobi\">Hello There!</p>,\n",
       " <p class=\"grievous\">General Kenobi!</p>,\n",
       " <p class=\"kenobi\">So Uncivilized!</p>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"p\") # Finds all p tags (returned as a list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `.get_text()` extracts the actual textual content within the tag (between `<p>` and `</p>` in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello There!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"p\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python lists\n",
    "\n",
    "Using `find_all()` always returns a Python list.\n",
    "\n",
    "A list is a standard Python data structure used to collect several elements in the same object.\n",
    "\n",
    "Lists are created using `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 29, 13, 41]\n"
     ]
    }
   ],
   "source": [
    "my_list = [2, 29, 13, 41]\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in a list is assigned an index starting from 0. One can call speciic elements in a list using the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(my_list[0]) # Print the first element\n",
    "\n",
    "print(my_list[3]) # Prints the last element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating over list elements\n",
    "\n",
    "When working with lists we often have to iterate over each element in list; that is repeated some function or command for each element. \n",
    "\n",
    "This can be done using a for loop. \n",
    "\n",
    "`.get_text()` works on a soup object and therefore not on returns from `find_all()`, as that returns a list. To extract the text from the contents of a list returned from `find_all()`, we have to iterate over the list elements (fx with a for loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello There!\n",
      "General Kenobi!\n",
      "So Uncivilized!\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"p\"):\n",
    "    print(tag.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code just prints out the textual content for each match.\n",
    "\n",
    "The text can be stored in a list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello There!', 'General Kenobi!', 'So Uncivilized!']\n"
     ]
    }
   ],
   "source": [
    "texts = [] # Creating empty list\n",
    "\n",
    "for tag in soup.find_all(\"p\"):\n",
    "    texts.append(tag.get_text()) # Adds each text to the list\n",
    "    \n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding HTML using attributes\n",
    "\n",
    "In addition to searching for tags, we can also specify attributes. Some attributes have arguments specific for them like id and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello There!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\", id = \"convo1\").get_text() # Search for a specific id attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `.get_text()` extracts *all* text within the tag including text within child tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for class attribute (notice the `_` added to `class_` as the `class` name is reserved somewhere else in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello There!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"p\", class_ = \"kenobi\").get_text() # Search for a specific class attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags can also be found by searching for the attribute alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello There!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_ = \"kenobi\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup supports a wide range of attributes (id, href, class). There are however no real rules as to what attributes can be called in HTML. BeautifulSoup therefore supports searching for any attribute with the following syntax:\n",
    "\n",
    "`attrs = {\"attribute\": \"value\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello There!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(attrs = {\"class\": \"kenobi\"}).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge check:\n",
    "\n",
    "What tags or attributes can be used to extract the text \"General Kenobi\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div id=\"convo1\">\n",
      "   <p class=\"kenobi\">\n",
      "    Hello There!\n",
      "   </p>\n",
      "  </div>\n",
      "  <div id=\"convo2\">\n",
      "   <p class=\"grievous\">\n",
      "    General Kenobi!\n",
      "   </p>\n",
      "  </div>\n",
      "  <div id=\"convo3\">\n",
      "   <p class=\"kenobi\">\n",
      "    So Uncivilized!\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the HTML structure\n",
    "\n",
    "Using `.find()` returns a new soup object (`.find_all()` a list of soup objects). Because these methods search for tags *within* the soup object, it is always child tags of the original soup that is returned.\n",
    "\n",
    "This allows one to parse further by first specifying one tag and then another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"kenobi\">Hello There!</p>\n"
     ]
    }
   ],
   "source": [
    "soup_child = soup.find(\"div\")\n",
    "\n",
    "soup_grandchild = soup_child.find(\"p\")\n",
    "\n",
    "print(soup_grandchild)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also allows one to navigate the structure, as the extracted soup objects maintains references to the HTML structure that it was extracted from.\n",
    "\n",
    "Using `.parent`, one can locate the tag in which a certain tag is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"kenobi\">Hello There!</p>\n",
      "<div id=\"convo1\"><p class=\"kenobi\">Hello There!</p></div>\n"
     ]
    }
   ],
   "source": [
    "soup_child = soup.find(\"p\", class_ = \"kenobi\")\n",
    "\n",
    "print(soup_child)\n",
    "\n",
    "print(soup_child.parent) # Returns the parent of soup_child (a div tag in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also iterate over all parents (and grand parents, so to speak) with `.parents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n",
      "body\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "for parent in soup_child.parents:\n",
    "    print(parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.next_sibling` and `.previous_sibling` you can navigate between tags on the same level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"convo1\"><p class=\"kenobi\">Hello There!</p></div>\n",
      "<div id=\"convo2\"><p class=\"grievous\">General Kenobi!</p></div>\n"
     ]
    }
   ],
   "source": [
    "soup_child = soup.find(\"div\")\n",
    "\n",
    "print(soup_child)\n",
    "\n",
    "print(soup_child.next_sibling) # Returns the next tag on the same level as soup_child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the right tags\n",
    "\n",
    "Let us try applying some of these skills on the European Union Climate Action news section.\n",
    "\n",
    "We already know how to get the HTML, so this just has to be converted to a soup object, and we are ready to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://nyheder.tv2.dk\")\n",
    "\n",
    "tv2_html = response.content\n",
    "\n",
    "tv2_soup = bs(tv2_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the right tags by just browing through raw HTML is not ideal.\n",
    "\n",
    "Instead we can use our browser to help us find the parts of the webpage to extract. Almost all browsers has an \"inspector tool\" of some kind that allows one to inspect the source code of a webpage (shortcut `F12` for a lot of browsers on Windows and `Command-Option-I` for Safari on Mac)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting news headlines TV2 Nyheder\n",
    "\n",
    "Inspecting the HTML of https://nyheder.tv2.dk, we see that the headlines are part of an \"a\" tag with the class \"o-teaser_link\". \n",
    "\n",
    "We can extract the first headline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida byder sig til som OL-vært i stedet for Tokyo\n"
     ]
    }
   ],
   "source": [
    "headline_soup = tv2_soup.find(\"a\", class_ = \"o-teaser_link\")\n",
    "print(headline_soup.get_text(strip = True)) # The arguement 'strip = True' removes leading and trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this tag and attribute also includes the \"Lige nu\" heading. If we wanted to avoid that, we have to change up the way we find the tags.\n",
    "\n",
    "Looking at the structure on the webpage, we see that headlines are within a \"section\" tag with the classes \"o-deck g-con g-col g-row_l g-gutter g-colx\". \n",
    "\n",
    "We could refer specifically to news headlines by first extracting the section tag and then the headline.\n",
    "\n",
    "*Note:* HTML class names cannot contain spaces, so when an HTML tag contains a class attribute that contains spaces, it is actually several classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamfundTidligere TV 2-direktør død\n"
     ]
    }
   ],
   "source": [
    "section_soup = tv2_soup.find(\"section\", class_ = \"o-deck g-con g-col g-row_l g-gutter g-colx\")\n",
    "headline_soup = section_soup.find(\"a\", class_ = \"o-teaser_link\")\n",
    "print(headline_soup.get_text(strip = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying this way, we are both getting the headline and the category. \n",
    "\n",
    "The headline itself is contained in a \"h2\" tag which we could extract instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidligere TV 2-direktør død\n"
     ]
    }
   ],
   "source": [
    "print(headline_soup.find(\"h2\").get_text(strip = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headline is also a link. Links are almost always created as an \"a\" tag.\n",
    "\n",
    "`headline_soup` is currently the soup object with the \"a\" tag containing the headline. Supposing we want to collect the links to the articles to scrape the articles themselves, we can extract that directly from this soup object.\n",
    "\n",
    "The URL linked is almost always stored as an \"href\" attribute in an \"a\" tag.\n",
    "\n",
    "Attributes can be extracted directly from soup objects using `[attribute]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//nyheder.tv2.dk/samfund/2021-01-26-tidligere-tv-2-direktoer-doed\n"
     ]
    }
   ],
   "source": [
    "headline_link = headline_soup['href']\n",
    "print(headline_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links can be either \"absolute\" or \"relative\". Absolute links contain the entire URL to access the page. A relative URL contains the path on the specific domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting all headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidligere TV 2-direktør død\n",
      "Britisk coronavariant blandt 12 procent af danske smittede\n",
      "44-årig mor til tre forsvandt uden at sige farvel, men nu finder tv-hold nye spor\n",
      "Tyskland tager Trumps vidundermiddel i brug\n",
      "EU vil stramme eksportregler efter ballade med forsinkede vacciner\n",
      "Overblik: Hvilke lande vaccinerer lige nu – og hvor mange i Danmark er blevet vaccineret?\n",
      "V-ordfører erkender, at kritiseret Dubai-tur også var ferie\n",
      "44-årig mor til tre forsvandt uden at sige farvel, men nu finder tv-hold nye spor\n",
      "LIVESverige stopper betaling til Pfizer over vaccinestrid\n",
      "Tyskland tager Trumps vidundermiddel i brug\n",
      "Tidligere TV 2-direktør død\n",
      "Hurtigtest er bedre end først antaget, viser dansk forskning\n",
      "Nyt udspil skal give flere tvangsfjernelser\n",
      "Minkformand hilser aftale velkommen - avlere kan stikke millioner i lommen, siger kommentator\n",
      "Han står nummer to i rækken efter vicepræsidenten - og har været med i fem Batman-film\n",
      "Her bliver anklageskriftet mod Trump overdraget til Senatet\n",
      "OnlyFans er pornoens nye guldkalv - men er det for let at være med?\n",
      "Min ven Roslyn er en af de mange millioner, der er skubbet ud i fattigdom på grund af coronavirus\n",
      "Flertal for at hastebehandle isolationskrav ved indrejse\n",
      "Vestens vaccine-egoisme er en trussel mod folkesundheden\n",
      "- Helt ekstraordinært og hans bedste landskamp nogensinde\n",
      "\"Løkkes værste mareridt\" vil kurere Venstre indefra\n",
      "Det ene sekund var han sportsstjerne, det næste var han lænket til en kørestol\n"
     ]
    }
   ],
   "source": [
    "section_soups = tv2_soup.find_all(\"section\", class_ = \"o-deck g-con g-col g-row_l g-gutter g-colx\")\n",
    "\n",
    "headline_soups = list()\n",
    "for section in section_soups:\n",
    "    section_headlines = section.find_all(\"a\", class_ = \"o-teaser_link\")\n",
    "    headline_soups = headline_soups + section_headlines\n",
    "    \n",
    "for headline_soup in headline_soups:\n",
    "    print(headline_soup.find(\"h2\").get_text(strip = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Headlines from politiken.dk\n",
    "\n",
    "Using what we have covered in the workshop, try to extract headlines from https://politiken.dk/.\n",
    "\n",
    "1. Extract the text of the first headline\n",
    "\n",
    "2. Extract the link to the article of the first headline\n",
    "\n",
    "3. Collect a list of the headlines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
