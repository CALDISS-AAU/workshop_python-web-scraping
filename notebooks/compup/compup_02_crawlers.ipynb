{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to crawlers/spiders in Python\n",
    "\n",
    "This notebook contains a short introduction to working with crawlers/spiders with `Scrapy`:\n",
    "\n",
    "- What are crawlers/spiders?\n",
    "- Defining functions in Python\n",
    "- What is a \"class\" in Python?\n",
    "- Building a simple crawler using `Scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are crawlers/spiders?\n",
    "\n",
    "Where \"web scraping\" refers to (mostly) automated collections of data and material from websites, crawlers and spiders are bots/programs specifically developed to traverse several websites and perform some scraping tasks.\n",
    "\n",
    "If we are interested in scraping the content of several websites without knowing the exact URLs of those websites, a crawler can be used to go from site to site and perform the necessary web scraping task.\n",
    "\n",
    "Developing crawlers can be especially tricky if they have to traverse several domains. This is because the web is connected in such a way where a few sites are dominant and are linked to across most websites (just think of how often you see links to Google, Twitter, Facebook etc. on a website). Imagining the web as an ocean with layers like the figure below, a crawler will always move towards the surface because the websites located there are referenced so often.\n",
    "\n",
    "Obviously we want to avoid the surface with a crawler, as it will then end up trying to crawl the entire web.\n",
    "\n",
    "![websea](./img/web_sea.png)\n",
    "\n",
    "*Source unknown*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a crawler\n",
    "\n",
    "The following should be considered when constructing a crawler:\n",
    "- Where should the crawler start?\n",
    "- What sites are of interest?\n",
    "- What scraping task should the crawler do?\n",
    "- How should the crawler be limited?\n",
    "\n",
    "In Python, the best way of constructing a crawler is to use relevant data structures to define the starting points and possible sites to avoid. The scraping tasks can be defined as functions to be integrated in the crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a scraper (using `Scrapy`)\n",
    "\n",
    "The package [`scrapy`](https://docs.scrapy.org/en/latest/) is used for various web scraping purposes. \n",
    "\n",
    "One major challenge when crawling is the massive amount of request-handling needed to crawl across various site (the crawler has to keep sending new requests and not just stop if it encounters a timeout). Another thing to be aware of is crawler-restrictions on the page (`robots.txt`) and avoiding sending too many requests to a server too quickly.\n",
    "\n",
    "Luckily `scrapy` has a lot of existing functions and classes that are created to account for common problems in scraping. Using scrapy, one can focus on the actual scraping tasks that needs to be performed.\n",
    "\n",
    "Here is a boiled down version of how to create a simple scraper using `scrapy`:\n",
    "- Create a crawler-class that is adapted from the base class `scrapy.Spider` (fx `my_crawler`)\n",
    "    - Name the spider by creating a `name` attribute (this is used to call it later)\n",
    "    - Specify the URLs to scrape in a `start_urls` attribute\n",
    "    - (Optional) Specify how the scraper should initially process the URLs in `start_urls` (by default, it sends a GET request for each and returns a response object)\n",
    "    - Specify how each response from the requests send should be processed by defining a `parse` function\n",
    "- Create a data structure for the scraped info to be stored in\n",
    "- Call the `CrawlerProcess()` from `scrapy`: `process = CrawlerProcess()`\n",
    "- Define what crawler the `CrawlerProcess()` should use: `process.crawl(my_crawler)`\n",
    "- Start the crawling: `process.start()`\n",
    "\n",
    "**NOTE ON RESTARTING CRAWLERS**\n",
    "\n",
    "A spacy crawler can only be run once in a given notebook instance. To restart the crawler, you have to restart the kernel of the notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eu_crawler(scrapy.spider): #intentional error to avoid mass crawling\n",
    "    name = \"eu_crawler\"\n",
    "    main_url = 'https://ec.europa.eu/clima/news-your-voice/news_en'\n",
    "    start_urls = ['https://ec.europa.eu/clima/news-your-voice/news_en']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        soup = bs(response.text, \"html.parser\") # Notice that HTML content is refered to as .text in a scrapy response\n",
    "        \n",
    "        article_rows_soup = soup.find_all(\"article\", class_ = \"ecl-content-item\")\n",
    "        \n",
    "        for row in article_rows_soup:\n",
    "            article_dict = {}\n",
    "\n",
    "            article_title_soup = row.find(\"div\", class_ = \"ecl-content-item__title\").find(\"a\")\n",
    "            article_title = article_title_soup.get_text()\n",
    "            article_link = article_title_soup['href']\n",
    "\n",
    "            article_date = row.find(\"time\")[\"datetime\"]\n",
    "\n",
    "            article_summary_soup = row.find(\"div\", class_ = \"ecl-content-item__description\")\n",
    "            try:\n",
    "                article_summary = article_summary_soup.get_text(strip = True)\n",
    "            except:\n",
    "                article_summary = \"\"\n",
    "\n",
    "            article_dict['title'] = article_title\n",
    "            article_dict['link'] = article_link\n",
    "            article_dict['date'] = article_date\n",
    "            article_dict['summary'] = article_summary\n",
    "\n",
    "            article_list.append(article_dict)\n",
    "        \n",
    "        try:\n",
    "            next_page_url = urljoin(self.main_url, soup.find(\"a\", attrs = {'aria-label': \"Go to next page\"})['href'])\n",
    "        except:\n",
    "            next_page_url = None\n",
    "            \n",
    "        if next_page_url is not None:\n",
    "            yield scrapy.Request(url = next_page_url, callback=self.parse)\n",
    "\n",
    "article_list = []\n",
    "process = CrawlerProcess(\n",
    "    {'USER_AGENT': 'Mozilla/5.0'}\n",
    ")\n",
    "process.crawl(eu_crawler)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
