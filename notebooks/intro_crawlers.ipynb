{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to crawlers/spiders in Python\n",
    "\n",
    "This notebook contains a short introduction to working with crawlers/spiders with `Scrapy`:\n",
    "\n",
    "- What are crawlers/spiders?\n",
    "- Defining functions in Python\n",
    "- What is a \"class\" in Python?\n",
    "- Building a simple crawler using `Scrapy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are crawlers/spiders?\n",
    "\n",
    "Where \"web scraping\" refers to (mostly) automated collections of data and material from websites, crawlers and spiders are bots/programs specifically developed to traverse several websites and performing some scraping tasks.\n",
    "\n",
    "If we are interested in scraping the content of several websites without knowing the exact URLs of those websites, a crawler can be used to go from site to site and perform the necessary web scraping task.\n",
    "\n",
    "Developing crawlers can be especially tricky if they have to traverse several domains. This is because the web is connected in such a way where a few sites are dominant and are linked to across most websites (just think of how often you see links to Google, Twitter, Facebook etc. on a website). Imagining the web as an ocean with layers like the figure below, a crawler will always move towards the surface because the websites located there are referenced so often.\n",
    "\n",
    "Obviously we want to avoid the surface with a crawler, as it will then end up trying to crawl the entire web.\n",
    "\n",
    "![websea](./img/web_sea.png)\n",
    "\n",
    "*Source unknown*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a crawler\n",
    "\n",
    "The following should be considered when constructing a crawler:\n",
    "- Where should the crawler start?\n",
    "- What sites are of interest?\n",
    "- What scraping task should the crawler do?\n",
    "- How should the crawler be limited?\n",
    "\n",
    "In Python, the best way of constructing a crawler is to use relevant data structures to define the starting points and possible sites to avoid. The scraping tasks can be defined as function to be integrated in the crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions in Python\n",
    "\n",
    "It is easy to define your own functions in Python. When working with crawlers, it makes sense to think of the scraping tasks to be performed as functions to be integrated.\n",
    "\n",
    "The syntax for defining a function is as follow:\n",
    "\n",
    "```{python}\n",
    "def my_function(x):\n",
    "    result = x + 2\n",
    "    return(result)\n",
    "```\n",
    "\n",
    "- `def` is used to define functions. \n",
    "- `my_function` is the name of the function. Make sure not to overwrite existing functions.\n",
    "- The `x` in parenthesis is the input argument. A function takes from 0 to any number of arguments separated by comma.\n",
    "- The indented lines following `:` are the commands for the function.\n",
    "- `result` is a variable created inside the function. It only exists inside the function. `x` refers to the input argument `x`.\n",
    "- `return` is used to specify the output of the function. Note: Commands following a return line in the function are ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(x):\n",
    "    result = x + 2\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that running the cell above returns nothing. We have simply defined a function which in itself does not have an output.\n",
    "\n",
    "The function is now available to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_function(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function adds 2 to the input argument (in this case `2`) and returns it (stored inside the function as `result`).\n",
    "\n",
    "Notice that `result` does not exist outside the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6459d04d738f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The return statement**\n",
    "\n",
    "If the function is meant to have an output, the return statement is used to specify what to be returned.\n",
    "\n",
    "Any lines following a return statement in a function is ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You see me!\n"
     ]
    }
   ],
   "source": [
    "def a_function():\n",
    "\n",
    "    print(\"You see me!\")\n",
    "    \n",
    "    return\n",
    "    \n",
    "    print(\"But you don't see me!\")\n",
    "    \n",
    "a_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "\n",
    "A integral part of Python programming is the ability to handle errors in the programming. \n",
    "\n",
    "In most data analysis tasks, we have little use for error handling. However, when working crawlers we are writing commands that have to be able to process information that we do not know before-hand. Error-handling can therefore be necessary to ensure that the crawler not just terminated when encountering an error.\n",
    "\n",
    "When we provide a function with an input that it is not able to handle, it will return an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7bbfd0f8d480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hello\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-0703a96d1b27>\u001b[0m in \u001b[0;36mmy_function\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "my_function(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the function performs addition on the input argument, Python returns an error because it cannot perform addition on text.\n",
    "\n",
    "In a crawler setting, the errors we could encounter could also be related to datatypes or trying to access attributs in HTML code that are not present.\n",
    "\n",
    "There are two main ways of including error handling:\n",
    "1. Using if-else statements\n",
    "2. Using try-except statements\n",
    "\n",
    "#### Using if-else statements as error handling\n",
    "\n",
    "if-else is simply used to write commands that should only be run when certain conditions are met:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_function(x):\n",
    "    if x > 2:\n",
    "        return(\"Above 2!\")\n",
    "    else:\n",
    "        return(\"Not above 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not above 2!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if_function(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If-else can be used to check the length of a data structure (number of elements), whether a certain attribute or tag is present and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using try-except as error handling\n",
    "\n",
    "Sometimes we may not know the specific conditions that have to be met to avoid the error but we instead know what errors could occur.\n",
    "try-except allows one to write commands that accounts for specific errors. \n",
    "\n",
    "The logic is as follows:\n",
    "\n",
    "- *try* to do something.\n",
    "- *except* if you encounter this error. Do something else.\n",
    "\n",
    "Below `my_function` is redefined to account for Type Errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_function(x):\n",
    "    try:\n",
    "        result = x + 2\n",
    "    except TypeError:\n",
    "        result = np.nan\n",
    "        \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_function(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function now no longer throws an error but instead returns a missing value (`np.nan`) when encountering a TypeError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Except all errors (beware)\n",
    "\n",
    "try-except statements do not need a specified error to work. It is possible to just except all errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(x):\n",
    "    try:\n",
    "        result = x + 2\n",
    "    except:\n",
    "        result = np.nan\n",
    "        \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_function(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule this practiced is highly discouraged as you run the risk of completely overlooking glaring errors in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(y):\n",
    "    try:\n",
    "        result = x + 2\n",
    "    except:\n",
    "        result = np.nan\n",
    "        \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_function(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function above, there is a mis-match between the input argument and the variable used in the function. Usually Python will throw a `NameError` when running a function like this but this error is captured by the `except` statement (as it captures all errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_scraper(start_urls, keywords):\n",
    "    # Create class\n",
    "    class kw_spider (scrapy.Spider):\n",
    "        name = \"kw_spider\"\n",
    "\n",
    "        def start_requests(self, start_urls = start_urls):\n",
    "            for start_url in start_urls:\n",
    "                #logger.info(\"Starting scrape for {start_url}...\".format(start_url = start_url))\n",
    "                yield scrapy.Request(url = start_url, callback = self.parse)\n",
    "\n",
    "        #Parsing\n",
    "        def parse(self, response):\n",
    "            print(response.url)\n",
    "\n",
    "            site_dict = {}\n",
    "\n",
    "            page_url = response.url\n",
    "            domain_url = urlparse(page_url).netloc\n",
    "            page_html = requests.get(page_url).content\n",
    "            page_soup = bs(page_html, 'html.parser')\n",
    "\n",
    "            page_text = page_soup.get_text().lower()\n",
    "\n",
    "            #try:\n",
    "            page_links = list(set([re.sub(r'\\/$', '', tag['href']) for tag in page_soup.find_all('a') if tag.has_attr('href')]))\n",
    "            page_links = list(compress(page_links, [len(link) > 1 for link in page_links]))\n",
    "\n",
    "            if any(word in page_text for word in keywords):\n",
    "                matches = list(compress(keywords, [keyword in page_text for keyword in keywords]))\n",
    "\n",
    "                site_dict['url'] = page_url\n",
    "                site_dict['links'] = page_links\n",
    "                site_dict['date-of-access'] = str(datetime.now().date())\n",
    "                site_dict['keywords_matched'] = matches\n",
    "\n",
    "                site_list.append(site_dict)\n",
    "\n",
    "            scraped_urls.append(page_url)\n",
    "\n",
    "            internal_urls = list(compress(page_links, [(domain_url in link or \"http\" not in link) for link in page_links]))\n",
    "\n",
    "            new_urls = list(set(internal_urls) - set(scraped_urls)) # Extracted URLs from pages - should be on same domain\n",
    "\n",
    "            if len(new_urls)>0:\n",
    "                more_pages = True # Test for whether there are more pages\n",
    "            else:\n",
    "                more_pages = False\n",
    "\n",
    "            #except:\n",
    "            #    more_pages = False       \n",
    "            \n",
    "            if more_pages:\n",
    "                for url in new_urls:\n",
    "                    yield scrapy.Request(url = urljoin(page_url, url), callback=self.parse)\n",
    "                        \n",
    "            \n",
    "            # Save scraped data\n",
    "            #outname = \"drr_scrape{}\".format(str(datetime.now().date()))\n",
    "            #with open(os.path.join(data_dir, outname), 'w') as f:\n",
    "            #    json.dump(site_list, f)\n",
    "                \n",
    "    #Initiatlize lists\n",
    "    site_list = list()\n",
    "    scraped_urls = list()\n",
    "\n",
    "    # Set parameters\n",
    "    start_urls = start_urls # start URLs\n",
    "    keywords = keywords # keywords\n",
    "\n",
    "    #Run spider\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(kw_spider)\n",
    "    process.start()\n",
    "    \n",
    "    return(site_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
